\documentclass[a4paper,12pt]{article}

%%% Packages %%%
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{url}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage[bottom,hang,flushmargin]{footmisc}

%%% Settings %%%
\setlength{\parindent}{0pt}
\frenchspacing
\urlstyle{same}
\MakeOuterQuote{"}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}
\renewcommand{\thefootnote}{\arabic{footnote}}

%%% Author, Title %%%
\title{Mathematical equations underlying \\ numerosity population receptive field models}
\author{Joram Soch}
\date{soch@cbs.mpg.de}

\begin{document}
	

\pagenumbering{roman}
\maketitle


\begin{abstract}
\noindent
We review the mathematical formulation of the statistical model used in "Topographic Representation of Numerosity in the Human Parietal Cortex" (Harvey et al., \textit{Science}, 2013; vol. 341, pp. 1123-1126; DOI: 10.1126/science.1239052) and provide actual equations for the entire model, with notation that is easy to follow and common in cognitive neuroscience. Since this study does not report any equations, we occassionally reference "Population receptive field estimates in human visual cortex" (Dumoulin \& Wandell, \textit{NeuroImage}, 2008; vol. 39, pp. 647-660; DOI: 10.1016/j.neuroimage.2007.09.034) which describes the modelling framework on which the analysis was based. In addition, we show how to simulate data from the generative model and how to extend the modelling approach with (i) inclusion of nuisance variables, (ii) accounting for hemodynamic variability and (iii) estimation via (restricted) maximum likelihood.
\end{abstract}


\pagebreak
\tableofcontents


%%% Text %%%
\pagebreak
\pagenumbering{arabic}
\section{Introduction} \label{sec:Intro}

Harvey and colleagues presented a computational model for numerosity processing in human cerebral cortex (Harvey et al., 2013), measured with functional magnetic resonance imaging (fMRI) and analyzed using a population receptive field (pRF) model for visual perception by Dumoulin and Wandell (Dumoulin \& Wandell, 2008). Briefly, measured fMRI signals were assumed to originate from neuronal activity following presented numerosity that follows log-Gaussian-shaped tuning functions, convolved with some hemodynamic response to yield a hemodynamic signal. This approach was later re-applied and extended by Harvey and colleagues (e.g. Harvey et al., 2011, 2015; Harvey \& Dumoulin, 2017; Hofstetter et al., 2021; Hofstetter \& Dumoulin, 2021; Cai et al., 2021a, 2021b, 2022, 2023; Paul et al., 2022).

Here, we review the statistical model formulation behind numerosity pRF models to facilitate simulation of fMRI signals compatible with specific tuning parameters in the context of our own research project on the EMergence of PRecISE numerosity representations in the human brain (EMPRISE).


\section{Model specification} \label{sec:Mod}

\subsection{Notation} \label{sec:Not}

An fMRI numerosity processing experiment has been performed. Let $x_t \in \mathcal{X}$ be the presented numerosity at continuous time $t$. Here, $\mathcal{X} = \left\lbrace 1, 2, \ldots, L, M \right\rbrace$ is the stimulus space where $L$ is the number of low numerosities levels (Harvey et al., 2013: $L = 7$; EMPRISE: $L = 5$) and $M$ is the high numerosity presented in intermissions (Harvey \& EMPRISE: $M = 20$\footnote{Harvey \& Dumoulin, 2017, p.~7: "The numerosities one to seven were first presented in ascending order, followed by 16.8 seconds showing 10 circles (sic!), followed (sic!) seven to one in descending order, followed by 16.8 seconds with 20 circles." However, this is incompatible with their Fig.~1 which shows a single high numerosity of 20.}; but see Cai et al., 2021b).

Furthermore, let $y_{ij}$ be the measured BOLD signal in a single voxel during the $i$-th scan (EMPRISE: $i = 1,\ldots,145$) belonging the $j$-th run (EMPRISE: $j = 1,\ldots,8$). We denote as $\bar{y}$ the signal which results from averaging across all runs and we denote as $\bar{y}_j$ the signal which results from averaing across repeated "epochs" of identical stimulus presentation within a run (Harvey \& EMPRISE: 4 epochs) and we denote as $\bar{\bar{y}}$  the signal which results from averaging across both, runs and epochs.


\subsection{Neuronal model}

We define a logarithmic Gaussian tuning function on stimulus $x$ as

\begin{equation} \label{eq:f-log}
f_\mathrm{log}(x) = \exp \left[ -\frac{1}{2} \frac{(\ln x - \mu_\mathrm{log})^2}{\sigma_\mathrm{log}^2} \right]
\end{equation}

\pagebreak
and a linear Gaussian tuning function (cf. Dumoulin \& Wandell, 2008, eq.~2\footnote{There is a typesetting error in this equation: The minus sign needs to be inside the parantheses.}) as 

\begin{equation} \label{eq:f-lin}
f_\mathrm{lin}(x) = \exp \left[ -\frac{1}{2} \frac{(x - \mu_\mathrm{lin})^2}{\sigma_\mathrm{lin}^2} \right]
\end{equation}

where $\exp(\mu_\mathrm{log})$ or $\mu_\mathrm{lin}$ corresponds to the preferred numerosity of the tuned population (see Appendix~\ref{sec:App-A}) and $\sigma_\mathrm{log}$ or $\sigma_\mathrm{lin}$ corresponds to the tuning width of the tuned population in logarithmic or linear stimulus space, respectively.

Instead of using the standard deviation $\sigma$, tuning functions are preferably parametrized in terms of the full width at half maximum (FWHM) which can be directly calculated from the standard deviation (see Appendix~\ref{sec:App-A}):

\begin{equation} \label{eq:fwhm-sigma}
\omega = \mathrm{FWHM} = \exp \left[ \mu_\mathrm{log} + \sqrt{2 \ln 2} \, \sigma_\mathrm{log} \right] - \exp \left[ \mu_\mathrm{log} - \sqrt{2 \ln 2} \, \sigma_\mathrm{log} \right] \; .
\end{equation}

Then, neuronal activity at time $t$ is assumed to be (cf. Dumoulin \& Wandell, 2008, eq.~3)

\vspace{-0.5em}
\begin{equation} \label{eq:z-t}
\begin{split}
z_t &= \sum_{x \in \mathcal{X}} \left[ x = x_t \right] \cdot f_\mathrm{log}(x) \\
&= f_\mathrm{log}(x_t)
\end{split}
\end{equation}

where the Iverson bracket notation $\left[ x = x_t \right]$ indicates presence of stimulus $x_t$ at time $t$.


\subsection{Hemodynamic model}

We define $h(t)$ as the hemodynamic response function (HRF) at post-stimulus time $t$, e.g. obtained from SPM using HRF parameters $\theta$:

\begin{equation} \label{eq:HRF}
h(t) \leftarrow \mathtt{spm\_hrf}(\theta) \; .
\end{equation}

Then, the hemodynamic signal at time $t$ is assumed to be a convolution of the neural activity with the HRF (cf. Dumoulin \& Wandell, 2008, eq.~4):

\vspace{-0.5em}
\begin{equation} \label{eq:s-t}
\begin{split}
s_t &= z_t \ast h(t) \\
&= \int_{0}^{t} z_\tau \cdot h(t-\tau) \, \mathrm{d}\tau \; .
\end{split}
\end{equation}

Finally, the measured hemodynamic signal is assumed to be a linear combination of the underlying hemodynamic signal and zero-mean normally distributed measurement noise (cf. Dumoulin \& Wandell, 2008, eq.~1\footnote{To be precise, Dumoulin and Wandell make no assumptions about the noise distribution.})

\begin{equation} \label{eq:y-s}
y = \beta_s \, s + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n
\end{equation}

where $y$ is an $n \times 1$ vector representing the measured BOLD signal ($n =$ number of scans), $s$ is an $n \times 1$ vector of the hemodynamic signal $s_t$, sampled at the acquisition times of $y$, $\beta_s$ is a scaling factor and $\sigma$ is the noise variance.


\pagebreak
\section{Model estimation} \label{sec:Est}

\subsection{Estimation of tuning parameters}

Let $\mu$ and $\omega$ be candidate tuning parameters (preferred numerosity and tuning width, i.e. FWHM) describing the potential neuronal response of a single voxel relative to presented stimuli $x_t$. We denote the resulting tuning function (see eq.~\ref{eq:f-log}) as $f_\mathrm{log}(x_t; \mu, \omega)$, the expected neuronal signal (see eq.~\ref{eq:z-t}) as $z_t(\mu,\omega)$ and the expected hemodynamic signal (cf. eq.~\ref{eq:y-s}) as $s(\mu,\omega)$.

Then, we can estimate the scaling factor by comparing the expected against the measured hemodynamic signal (here, assuming 1 run only)

\begin{equation} \label{eq:bs-est}
\hat{\beta}_s(\mu,\omega) = \operatorname*{arg\,min}_{\beta_s} \sum_{i=1}^{n} \left[ y_i - \beta_s s_i(\mu,\omega) \right]^2
\end{equation}

and calculate the residual sum of squares (RSS) as a function of the estimated scaling factor (cf. Dumoulin \& Wandell, 2008, eq.~5):

\begin{equation} \label{eq:RSS}
\mathrm{RSS}(\mu,\omega) = \sum_{i=1}^{n} \left[ y_i - \hat{\beta}_s(\mu,\omega) s_i(\mu,\omega) \right]^2 \; .
\end{equation}

The estimated preferred numerosity $\hat{\mu}$ and estimated tuning width $\hat{\omega}$ are then obtained as those candidate tuning parameters that minimize the RSS:

\begin{equation} \label{eq:mu-tw-est}
(\hat{\mu},\hat{\omega}) = \operatorname*{arg\,min}_{(\mu,\omega)} \mathrm{RSS}(\mu,\omega) \; .
\end{equation}


\subsection{Estimation of hemodynamic parameters}

In order to account for different hemodynamic response functions between subjects, but not between regions or voxels, Harvey and colleagues report (Harvey et al., 2013, suppl. p.~9; Harvey \& Dumoulin, 2017, p.~7) to have estimated subject-wise HRF parameters after an initial estimation of tuning parameters (which was based on default HRF parameters) and then re-estimating tuning parameters (using the updated HRF parameters). This estimation scheme can be summarized as follows\footnote{It is obvious that the third line of these equations is dubious and needs clarification. For an implementation, see here: \url{https://github.com/SkeideLab/EMPRISE/blob/akieslinger/data/code/harveymodel.py#L549} (function "estimate\_hrf").}:

\begin{equation} \label{eq:Harvey}
\begin{split}
h_0 &\leftarrow \mathtt{spm\_hrf}(\theta_0) \\
(\hat{\mu}_0,\hat{\omega}_0) &= \operatorname*{arg\,min}_{(\mu,\omega)} \mathrm{RSS}(\mu,\omega;h_0) \\
\hat{\theta} &\leftarrow \left\lbrace y, x, \hat{\mu}_0, \hat{\omega}_0 \right\rbrace \\
h &\leftarrow \mathtt{spm\_hrf}(\hat{\theta}) \\
(\hat{\mu},\hat{\omega}) &= \operatorname*{arg\,min}_{(\mu,\omega)} \mathrm{RSS}(\mu,\omega;h)
\end{split}
\end{equation}

where $\theta_0$ are the default HRF parameters and subscript 0 signifies initial estimates.


\subsection{Inclusion of nuisance regressors}

It is very unlikely that equation (\ref{eq:y-s}) is a realistic model of preprocessed, but otherwise unaltered fMRI signal. In particular, this model does not account for a baseline level of hemodynamic activity and does not allow for experimental conditions of no interest and nuisance variables, possibly influencing the preprocessed signal. We assume that these factors can be summarized as a design matrix

\begin{equation} \label{eq:X}
X = \left[ \begin{matrix} s(\mu,\omega) & X_c & 1_n \end{matrix} \right]
\end{equation}

where $s(\mu,\omega)$ is identical to the $s$ in equation (\ref{eq:y-s}), $X_c$ is an $n \times c$ matrix of confounds and $1_n$ is an $n \times 1$ vector of ones, such that the model from (\ref{eq:y-s}) changes to

\begin{equation} \label{eq:y-X}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n) \; .
\end{equation}

Then, regression coefficients can be estimated using ordinary least squares (OLS)

\begin{equation} \label{eq:b-est-X}
\hat{\beta}(\mu,\omega) = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y
\end{equation}

and estimated tuning parameters would be given as minimizing the RSS:

\begin{equation} \label{eq:mu-tw-est-X}
(\hat{\mu},\hat{\omega}) = \operatorname*{arg\,min}_{(\mu,\omega)} \left[ y - X \hat{\beta}(\mu,\omega) \right]^\mathrm{T} \left[ y - X \hat{\beta}(\mu,\omega) \right] \; .
\end{equation}

In the case that the numerosity tuning model is estimated from signals averaged over runs or epochs, run-wise nuisance variables should be regressed out of run-wise signals before averaging. If the measured signal of the $j$-th run is denoted as $y_j$ and the confound variables for the $j$-th run are given by (cf. eq.~\ref{eq:X})

\begin{equation} \label{eq:X-j}
X_j = \left[ \begin{matrix} X_{c,j} & 1_n \end{matrix} \right] \; ,
\end{equation}

then regression coefficients for the confound variables can be estimated as

\begin{equation} \label{eq:b-est-X-j}
\left[ \begin{matrix} \hat{\beta}_{c,j} \\ \hat{\beta}_{0,j} \end{matrix} \right] = (X_j^\mathrm{T} X_j)^{-1} X_j^\mathrm{T} y_j \; ,
\end{equation}

the confound-subtracted signal from the $j$-th run is calculated as

\begin{equation} \label{eq:y-j-reg}
y_j^{*} = y_j - X_{c,j} \hat{\beta}_{c,j}
\end{equation}

and the averaged signal across runs (see p.~\pageref{sec:Not}) would be given by

\begin{equation}  \label{eq:y-avg}
\bar{y} = \frac{1}{r} \sum_{j=1}^{r} y_j^{*}
\end{equation}

to which numerosity tuning parameter estimation is then applied:

\begin{equation} \label{eq:y-X-avg}
\bar{y} = \left[ \begin{matrix} s(\mu,\omega) & 1_n \end{matrix} \right] \left[ \begin{matrix} \beta_s \\ \beta_0 \end{matrix} \right] + \varepsilon \; .
\end{equation}


\subsection{Inclusion of hemodynamic derivatives} \label{sec:HRF-der}

Another way to account for variability in the hemodynamic response, also between voxels, is to include HRF derivatives into the model. Let $h'(t)$ be the time derivative and let $h''(t)$ be the dispersion deriative of the canonical HRF\footnote{Note: Whereas the time derivative does in fact correspond to the first derivative, the dispersion derivative is not equal to the second deriative of the canonical HRF, but corresponds to the derivative with respect to the dispersion parameter of the canonical HRF; see: \url{https://github.com/spm/spm12/blob/master/spm_get_bf.m#L144} (section "add dispersion deriative").}\textsuperscript{,}\footnote{For an implementation, see here: \url{https://github.com/SkeideLab/EMPRISE/blob/JoramSoch/code/Python/PySPMs.py#L401} (function "spm\_get\_bf").}:

\begin{equation} \label{eq:dHRF}
\left\lbrace h(t), \, h'(t), \, h''(t) \right\rbrace \leftarrow \mathtt{spm\_get\_bf}(\theta) \; .
\end{equation}

Then, we obtain three hemodynamic signals (cf. eq.~\ref{eq:s-t})

\vspace{-0.5em}
\begin{equation} \label{eq:s-ds-d2s-t}
\begin{split}
s_t &= z_t \ast h(t) \\
s'_t &= z_t \ast h'(t) \\
s''_t &= z_t \ast h''(t)
\end{split}
\end{equation}

which can be combined in the predictive linear model (cf. eq.~\ref{eq:y-s})

\begin{equation} \label{eq:y-S}
y = S \beta_S + \varepsilon, \; \varepsilon_i \sim \mathcal{N}(0, \sigma^2), \; i = 1,\ldots,n
\end{equation}

or, when including nuisance regressors, the design matrix is (cf. eq.~\ref{eq:X})

\begin{equation} \label{eq:X-S}
X(\mu,\omega) = \left[ \begin{matrix} S(\mu,\omega) & X_c & 1_n \end{matrix} \right] \quad \text{where} \quad S(\mu,\omega) = \left[ \begin{matrix} s & s' & s'' \end{matrix} \right]
\end{equation}

with $s'$ and $s''$ as temporally downsampled $n \times 1$ vector representations of $s'_t$ and $s''_t$. Due to their nature of being deriatives with respect to time and dispersion, $s'$ and $s''$ allow to capture voxel-by-voxel modulations in hemodynamic response time and resonse dispersion -- which are the most prominent forms of hemodynamic variability observed in empirical data (Zeidman et al., 2018, Fig.~3; Morante et al., 2021, Fig.~4).

Once again, if the numerosity tuning model is estimated from run-averaged signals, $S(\mu,\omega)$ would be applied to signals that have been cleaned from confound variables before averaging across runs, such that $X_c$ in equation (\ref{eq:X-S}) could be dropped.


\pagebreak
\subsection{Maximum likelihood estimation}

Given that stimulus signals for fixed $\mu$ and $\omega$, nuisance regressors and implicit baseline are available in the form of a design matrix $X$ (cf. eq.~\ref{eq:X-S}), tuning parameters -- and possibly, hemodynamic parameters -- can also be obtained via maximum likelihood estimation (MLE). Equation \eqref{eq:y-X} implies the following log-likelihood function:

\begin{equation} \label{eq:LLF}
\begin{split}
\mathrm{LL}(X,\beta,\sigma^2) &= \log p(y|X,\beta,\sigma^2) = \log \mathcal{N}(y; X\beta, \sigma^2 I_n) \\
&= \log \left( \sqrt{\frac{1}{(2\pi)^n |\sigma^2 I_n|}} \cdot \exp\left[ -\frac{1}{2} (y - X\beta)^\mathrm{T} (\sigma^2 I_n)^{-1} (y - X\beta) \right] \right) \\
&= - \frac{n}{2} \log(2\pi) - \frac{n}{2} \log (\sigma^2) - \frac{1}{2 \sigma^2} (y - X\beta)^\mathrm{T} (y - X\beta) \; .
\end{split}
\end{equation}

Maximizing this function with respect to $\beta$ and $\sigma^2$ gives rise to the ML estimates

\vspace{-0.5em}
\begin{equation} \label{eq:MLE}
\begin{split}
\hat{\beta} &= \operatorname*{arg\,max}_{\beta} \mathrm{LL}(X,\beta,\sigma^2) = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \\
\hat{\sigma}^2 &= \operatorname*{arg\,max}_{\sigma^2} \mathrm{LL}(X,\hat{\beta},\sigma^2) = \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} (y-X\hat{\beta})
\end{split}
\end{equation}

and the maximum log-likelihood (MLL) for given tuning parameters $\mu$ and $\omega$

\begin{equation} \label{eq:MLL}
\mathrm{MLL}(\mu,\omega) = \mathrm{LL}(X[\mu,\omega],\hat{\beta},\hat{\sigma}^2) = - \frac{n}{2} \log\left( \frac{\mathrm{RSS}\left[ \mu, \omega \right]}{n} \right) - \frac{n}{2} \log(2\pi) - \frac{n}{2} \; ,
\end{equation}

such that ML estimates of the tuning parameters would be given by

\begin{equation} \label{eq:mu-tw-est-ML}
(\hat{\mu},\hat{\omega}) = \operatorname*{arg\,max}_{(\mu,\omega)} \mathrm{MLL}(\mu,\omega) \; ,
\end{equation}

i.e. as those values of $\mu$ and $\omega$ which maximize the maximum log-likelihood. Since MLL is a monotonically decreasing function of RSS, maximizing the log-likelihood is equivalent to minimizing the residual sum of squares (see Appendix~\ref{sec:App-B}).

Moreover, as there is a direct relationship between maximum log-likelihood and residual sum of squares (see eq.~\ref{eq:MLL}), the coefficient of determination ($R^2$, "R-squared") can be directly calculated from the maximized MLL (see Appendix~\ref{sec:App-C})

\begin{equation} \label{eq:R2-MLL}
R^2 = 1 - \left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n}
\end{equation}

where $\Delta\mathrm{MLL}$ refers to the difference in maximum log-likelihood between the numerosity tuning model including the tuning parameters $\mu$ and $\omega$ and a model only including a constant regressor for the implicit baseline.


\pagebreak
\subsection{Restricted maximum likelihood}

Another limitation of (\ref{eq:y-s}) consists in the fact that it assumes independent and identically distributed (i.i.d.) error terms $\varepsilon_i$ and thus does not allow for error covariance which typically exists in the form of serial correlations in fMRI. We assume that such serial correlations are given in the form of a temporal covariance matrix $V$, e.g. obtained using SPM's restricted maximum likelihood (ReML) approach:

\begin{equation} \label{eq:ReML}
V \leftarrow \mathtt{spm\_reml}(Y, X)
\end{equation}

where $Y$ is an $n \times v$ matrix of fMRI signals across all voxels inside a region of interest (ROI) or in the whole brain, such that the model from (\ref{eq:y-s}) changes to

\begin{equation} \label{eq:y-X-V}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

Then, regression coefficients can be estimated using weighted least squares (WLS)

\begin{equation} \label{eq:b-est-X-V}
\hat{\beta}(\mu,\omega) = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y
\end{equation}

and estimated tuning parameters would be given as minimizing the weighted RSS:

\begin{equation} \label{eq:mu-tw-est-X-V}
(\hat{\mu},\hat{\omega}) = \operatorname*{arg\,min}_{(\mu,\omega)} \left[ y - X \hat{\beta}(\mu,\omega) \right]^\mathrm{T} V^{-1} \left[ y - X \hat{\beta}(\mu,\omega) \right] \; .
\end{equation}

Maximum likelihood estimation under such correlated observations would proceed by maximizing the log-likelihood function $\mathrm{LL}(X,V,\beta,\sigma^2)$ -- now also a function of the temporal non-sphericity matrix $V$ -- which gives rise to the same parameter estimates

\begin{equation} \label{eq:MLE-V}
\hat{\beta} = \operatorname*{arg\,max}_{\beta} \mathrm{LL}(X,V,\beta,\sigma^2) = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y
\end{equation}

and the maximum log-likelihood as a function of the tuning parameters

\begin{equation} \label{eq:MLL-V}
\mathrm{MLL}(\mu,\omega) = - \frac{n}{2} \log\left( \frac{\mathrm{wRSS}\left[ \mu, \omega \right]}{n} \right) - \frac{n}{2} \log(2\pi) - \frac{n}{2} - \frac{1}{2} \log|V| \; .
\end{equation}

Once again, since the MLL is uniquely related to the weighted RSS (see Appendix~\ref{sec:App-B}), maximum likelihood estimation will give the same tuning parameter estimates as estimation that minimizes the weighted sum of squares -- as long as both are accounting for the same correlation structure $V$.


\pagebreak
\subsection{Grid-based parameter search}

Although maximum log-likelihood (see eqs.~\ref{eq:MLL}/\ref{eq:MLL-V}) and residual sum of squares (see eqs.~\ref{eq:mu-tw-est-X}/\ref{eq:mu-tw-est-X-V}) are both functions of the tuning parameters $\mu$ and $\omega$, there is no simple functional form of the former in terms of the latter. This due to the transformations that happen between the neuronal signal $z_t$ (see eq.~\ref{eq:z-t}) and the hemodynamic regressor $s$ (see eq.~\ref{eq:y-s}), i.e. discrete convolution and temporal downsampling.

For this reason, we cannot directly calculate estimates of $\mu$ and $\omega$ from the measured data $y$. Instead, the strategy is to perform an exhaustive parameter search within a plausible grid of reasonable values at a sufficient precision:

\vspace{-0.5em}
\begin{equation} \label{eq:fgs-fwhm}
\begin{split}
\mu \in \mathrm{M} &= \left\lbrace \mu_\mathrm{min}, \, \mu_\mathrm{min} + \Delta \mu, \, \ldots, \, \mu_\mathrm{max} - \Delta \mu, \, \mu_\mathrm{max} \right\rbrace \\
\omega \in \Omega &= \left\lbrace \omega_\mathrm{min}, \, \omega_\mathrm{min} + \Delta \omega, \, \ldots, \, \omega_\mathrm{max} - \Delta \omega, \, \omega_\mathrm{max} \right\rbrace \; .
\end{split}
\end{equation}

Alternatively, the set of plausible values for the tuning width may also be parametrized in terms of $\sigma_\mathrm{log}$, the standard deviation in logarithmic numerosity space (cf. eq. \ref{eq:f-log}):

\vspace{-0.5em}
\begin{equation} \label{eq:fgs-sig}
\begin{split}
\mu \in \mathrm{M} &= \left\lbrace \mu_\mathrm{min}, \, \mu_\mathrm{min} + \Delta \mu, \, \ldots, \, \mu_\mathrm{max} - \Delta \mu, \, \mu_\mathrm{max} \right\rbrace \\
\sigma_\mathrm{log} \in \Sigma &= \left\lbrace \sigma_\mathrm{min}, \, \sigma_\mathrm{min} + \Delta \sigma, \, \ldots, \, \sigma_\mathrm{max} - \Delta \sigma, \, \sigma_\mathrm{max} \right\rbrace \; .
\end{split}
\end{equation}

Note that constant candidate values for $\sigma_\mathrm{log}$ have the consequence that the candidate values of $\omega$ are different between different candidate values of $\mu$ (see eq.~\ref{eq:fwhm-sigma} and Appendix~\ref{sec:App-A}). However, since tuning width is positively related with preferred numerosity in numerosity-selective brain regions (Harvey et al., 2013, Fig.~4B; Harvey \& Dumoulin, 2017, Fig.~4a), it makes sense to vary the FWHM with preferred numerosity, such that higher values of FWHM are tested for higher preferred numerosity.


\subsection{Implementation in EMPRISE}

For functional MRI data measured within EMPRISE, we perform averaging across runs, but no averaging across epochs, since this gave best results in a simulation study prior to data analysis. Consequently, nuisance variables (motion parameters, global signals, temporal drifts\footnote{For default covariates, see here: \url{https://github.com/SkeideLab/EMPRISE/blob/JoramSoch/code/Python/EMPRISE.py#L113-L115} (variable "covs").}) are regressed out before averaging across runs (see eqs.~\ref{eq:X-j}-\ref{eq:y-X-avg}).

Moreover, we assume i.i.d. errors, because we found that pilot results were not qualitatively different when serial correlations were estimated using restricted maximun likelihood (see eqs.~\ref{eq:ReML}-\ref{eq:mu-tw-est-X-V}). Finally, we used no HRF derivatives, since we found that estimated tuning parameters were more noisy when including them (see eqs.~\ref{eq:dHRF}-\ref{eq:X-S}). In summary, this model is referred to as "True [averaging across runs], False [no averaging across epochs], iid [no restricted maximum likelihood], 1 [only canonical HRF regressor]"\footnote{For estimation settings, see here: \url{https://github.com/SkeideLab/EMPRISE/blob/JoramSoch/code/Python/NumpRF.py#L539-L550} (function "estimate\_MLE").}.

\pagebreak
For the tuning parameter grid, we consider preferred numerosities between 0.8 and 5.2 in steps of 0.05 as well as 20 (90 values) and logarithmic tuning widths between 0.05 and 3 in steps of 0.05 (60 values)\footnote{This is referred to as "Version 2" in the current implementation of numerosity analysis; see: \url{https://github.com/SkeideLab/EMPRISE/blob/JoramSoch/code/Python/EMPRISE.py#L728-L735} (function "analyze\_numerosity").}:

\vspace{-0.5em}
\begin{equation} \label{eq:fgs-EMPRISE}
\begin{split}
\mu_\mathrm{min} &= 0.8, \; \mu_\mathrm{max} = 5.2, \; \Delta \mu = 0.05 \\
\sigma_\mathrm{min} &= 0.05, \; \sigma_\mathrm{max} = 3, \; \Delta \sigma = 0.05 \; .
\end{split}
\end{equation}

Note that this results in different FWHM tuning widths per preferred numerosity (see Figure~\ref{fig:TP}). The total parameter space then is the cartesian product of these two sets

\vspace{-0.5em}
\begin{equation} \label{eq:Theta}
\begin{split}
(\mu,\sigma_\mathrm{log}) \in \Theta \hphantom{|} &= \mathrm{M} \times \Sigma \\
|\Theta| &= \text{number of possible tuning parameters} \\
&\quad\; \text{(here: } 90 \times 60 = 5{,}400 \text{ values)}
\end{split}
\end{equation}

and all optimization (minimizing or maximizing) will always address this set.

\vspace{1em}
\begin{figure}[h]
	% Figure TP
	\begin{center}
		\includegraphics[width=1.0\linewidth, clip=true, trim=80 40 80 40]{../../Figures/Figures/Figure_TP}
	\end{center}
	\vspace{-1em}
	\caption{\textit{Tuning parameter grid used for numerosity analysis.} Scatter plot of possible values of FWHM tuning width $\omega$ (x-axis) against possible values of preferred numerosity $\mu$ (y-axis), as implied by the grid defined in equation (\ref{eq:fgs-sig}) and the values given by equation (\ref{eq:fgs-EMPRISE}). Each blue dot corresponds to one set of parameters for which the scaling factor $\beta$ is estimated by maximizing the log-likelihood function (see Section~\ref{sec:Est}).} \label{fig:TP}
\end{figure}



\pagebreak
\section{Data simulation} \label{sec:Sim}

\subsection{Specification of simulation parameters}

In order to simulate voxel-wise multi-run fMRI signals compatible with a numerosity experimental design such as EMPRISE, the following parameters need to be specified:

\begin{itemize}
	
\item preferred numerosity $\mu_k$ and FWHM tuning width $\omega_k$ for each voxel $k$ where $k = 1,\ldots,v$ and $v$ is the number of voxels;

\item mean signal coefficients $\mu_s$ for signal regressors (numerosity signal and implicit baseline) and mean signal coefficients $\mu_c$ for confound regressors (cf. eq.~\ref{eq:X});

\item between-voxel variance $\sigma_k^2$;

\item between-run variance $\sigma_j^2$;

\item within-run variance $\sigma_i^2$;

\item time constant $\tau$.

\end{itemize}

Furthermore, we assume that the following quantities are available:

\begin{itemize}

\item a hemodynamic response function $h(t)$ (see eq.~\ref{eq:HRF});

\item a set of presented numerosities $x_{t,j}$ (see Section~\ref{sec:Mod}.1) at time $t$ in each run $j$ where $j = 1,\ldots,r$ and $r$ is the number of runs;

\item an $n \times c$ matrix of confound regressors $X_{c,j}$ for each run where $n$ is the number of scans and $c$ is the number of variables.
	
\end{itemize}


\subsection{Sampling of simulated signals}

Based on these choices, synthetic data are then generated as follows (see Figures~\ref{fig:GM-A}/\ref{fig:GM-B}): First, presented numerosities $x_t$, preferred numerosity $\mu$ and FWHM tuning width $\omega$ from a single voxel are used to generate neuronal signals according to equation (\ref{eq:z-t}). These neuronal signals are then convolved with the hemodynamic response function $h(t)$ according to equation (\ref{eq:s-t}) and, with the confound regressors $X_{c,j}$, combined into the design matrix $X_j$ for a single run according to equation (\ref{eq:X}):

\begin{equation} \label{eq:Xj}
X_j = \left[ \begin{matrix} s_j(\mu,\omega) & X_{c,j} & 1_n \end{matrix} \right]
\end{equation}

Second, voxel-wise regression coefficients are sampled as

\vspace{-0.5em}
\begin{equation} \label{eq:b}
\begin{split}
\beta_s &\sim \mathcal{N}(\mu_s, \sigma_k^2) \quad \text{for stimulus regressor and implicit baseline} \\
\beta_c &\sim \mathcal{N}(\mu_c, \sigma_k^2) \quad \text{for each confound regressor}
\end{split}
\end{equation}

and run-wise regression coefficients are sampled as

\vspace{-0.5em}
\begin{equation} \label{eq:bj}
\begin{split}
\beta_{s,j} &\sim \mathcal{N}(\beta_s, \sigma_j^2) \quad \text{for each run } j = 1,\ldots,r \\
\beta_{c,j} &\sim \mathcal{N}(\beta_c, \sigma_j^2) \quad \text{for each run } j = 1,\ldots,r
\end{split}
\end{equation}

and $\beta_{s,j}$ and $\beta_{c,j}$ are then combined into an $(1+c+1) \times 1$ vector (cf. eq.~\ref{eq:Xj}) of regression coefficients $\beta_j$ for a single run.

\pagebreak
Third, a scan-by-scan covariance matrix implying a temporal auto-correlation structure is generated as

\begin{equation} \label{eq:V}
V = \left[ \begin{matrix}
\tau^0 & \tau^1 & \cdots & \tau^{n-2} & \tau^{n-1} \\
\tau^1 & \tau^0 & \tau^1 & \ddots & \tau^{n-2} \\
\vdots & \tau^1 & \tau^0 & \tau^1 & \vdots \\
\tau^{n-2} & \ddots & \tau^1 & \tau^0 & \tau^1 \\
\tau^{n-1} & \tau^{n-2} & \cdots & \tau^1 & \tau^0 \\
\end{matrix} \right] \quad \text{where} \quad 0 < \tau < 1
\end{equation}

and noise terms for each run are sampled from the multivariate normal distribution with mean zero and covariance matrix as a product of the within-run variance and the temporal correlation matrix:

\begin{equation} \label{eq:ej}
\varepsilon_j \sim \mathcal{N}(0, \sigma_i^2 \, V)
\end{equation}

Note that this is equivalent to sampling each error term from the univariate normal distribution with mean zero and the noise variance, if $\tau$ is very close to zero, such that $V$ becomes close to the identity matrix:

\begin{equation} \label{eq:eij}
\varepsilon_{ij} \sim \mathcal{N}(0, \sigma_i^2), \; i = 1,\ldots,n, \; \text{if } \tau \rightarrow 0, \; \text{s.t. } V \approx I_n \; .
\end{equation}

Finally, the run-wise design matrix $X_j$ from equation (\ref{eq:Xj}), regression coefficients $\beta_j$ from equation (\ref{eq:bj}) and noise terms $\varepsilon_j$ from equation (\ref{eq:ej}) are used to generate simulated signals $y_j$ for a single run according to equation (\ref{eq:y-X-V}):

\begin{equation} \label{eq:yj}
y_j = X_j \beta_j + \varepsilon_j \; .
\end{equation}

This procedure is repeated for each voxel and each run to generate an $n \times v \times r$ (scan-by-voxel-by-run) array of simulated fMRI signals.

% \pagebreak
\begin{figure}
	% Figure GM-A
	\begin{center}
		\includegraphics[width=1.0\linewidth, clip=true, trim=20 40 20 40]{../../Figures/Figures/Figure_GM_A}
	\end{center}
	\vspace{-1em}
	\caption{\textit{Graphical model illustrating the generative model.} This figure illustrates the statistical model underlying data simulation in a single voxel. Squares correspond to fixed quantities and circles correspond to random variables. Arrows denote functional dependence (e.g. $\tau \rightarrow V$, cf. eq.~\ref{eq:V}) or that the probability distribution of the target quantity is parametrized in terms of the source quantities (e.g. $\left\lbrace \sigma_i^2, V \right\rbrace \rightarrow \varepsilon_j$, cf. eq.~\ref{eq:ej}). Everything inside the large box is run-dependent and everything inside the small box is additionally scan-dependent.} \label{fig:GM-A}
	% Figure GM-B
	\begin{center}
		\includegraphics[width=1.0\linewidth, clip=true, trim=20 60 20 40]{../../Figures/Figures/Figure_GM_B}
	\end{center}
	\vspace{-1em}
	\caption{\textit{Graphical model illustrating the generative model.} This figure is a simplified version of Figure~\ref{fig:GM-A} in which (i) regression coefficients $\beta_j$ are not differentiated between signal and confound regressors and (ii) creation of the run-wise design matrix $X_j$ is explained more transparently.} \label{fig:GM-B}
\end{figure}



%%% Appendix %%%
\pagebreak
\section{Appendix} \label{sec:Appendix}
\renewcommand\thesubsection{\Alph{subsection}}
\renewcommand\theequation{\thesubsection.\arabic{equation}}

\setcounter{equation}{0}
\subsection{Maximum and width of logarithmic tuning function} \label{sec:App-A}

Let $f_\mathrm{log}(x; \mu_\mathrm{log}, \sigma_\mathrm{log})$ be the Gaussian tuning function to a linear stimulus $x$ in logarithmic space (see eq.~\ref{eq:f-log}). Given tuning parameters $\mu_\mathrm{log}$ and $\sigma_\mathrm{log}$, we are interested in the preferred stimulus $\mu$ (= the value at which $f_\mathrm{log}(x)$ reaches its maximum) and the tuning width $\omega$ (= the FWHM of  $f_\mathrm{log}(x)$).

The first two derivatives of the logarithmic tuning function are:

\begin{equation} \label{eq:df-log}
f'_\mathrm{log}(x) = \left[ -\frac{1}{x \, \sigma_\mathrm{log}^2} (\ln x - \mu_\mathrm{log}) \right] \cdot \exp \left[ -\frac{1}{2} \frac{(\ln x - \mu_\mathrm{log})^2}{\sigma_\mathrm{log}^2} \right]
\end{equation}

\begin{equation} \label{eq:d2f-log}
f''_\mathrm{log}(x) = \left[ -\frac{1}{x^2 \, \sigma_\mathrm{log}^2} \left( -\frac{1}{\sigma_\mathrm{log}^2} (\ln x - \mu_\mathrm{log})^2 + (\ln x - \mu_\mathrm{log} +1) \right) \right] \cdot \exp \left[ -\frac{1}{2} \frac{(\ln x - \mu_\mathrm{log})^2}{\sigma_\mathrm{log}^2} \right] \; .
\end{equation}

Setting the first derivative to zero, we obtain:

\vspace{-0.5em}
\begin{equation} \label{eq:mu-qed}
\begin{split}
f'_\mathrm{log}(\mu) &= 0 \\
0 &= \left[ -\frac{1}{\mu \, \sigma_\mathrm{log}^2} (\ln \mu - \mu_\mathrm{log}) \right] \cdot \exp \left[ -\frac{1}{2} \frac{(\ln \mu - \mu_\mathrm{log})^2}{\sigma_\mathrm{log}^2} \right] \\
0 &= -\frac{1}{\mu \, \sigma_\mathrm{log}^2} (\ln \mu - \mu_\mathrm{log}) \\
0 &= \ln \mu - \mu_\mathrm{log} \\
\mu &= \exp \left[ \mu_\mathrm{log} \right] \; .
\end{split}
\end{equation}

Since the second deriative at this value is negative

\begin{equation} \label{eq:d2f-log-mu}
f''_\mathrm{log}(\mu) = -\frac{1}{\left( \exp \left[ \mu_\mathrm{log} \right] \sigma_\mathrm{log} \right)^2} < 0 \; ,
\end{equation}

there is a maximum at $\mu$. The function value at $\mu$ is

\begin{equation} \label{eq:f-log-mu}
\begin{split}
f_\mathrm{log}(\mu) &= \exp \left[ -\frac{1}{2} \frac{(\ln \exp \left[ \mu_\mathrm{log} \right] - \mu_\mathrm{log})^2}{\sigma_\mathrm{log}^2} \right] \\
&= \exp \left[ 0 \right] = 1 \; .
\end{split}
\end{equation}

\pagebreak
Thus, we are searching for the values of $x$ that satisfy

\vspace{-0.5em}
\begin{equation} \label{eq:fwhm-x12}
\begin{split}
f_\mathrm{log}(x_{1/2}) &= \frac{1}{2} \\
\frac{1}{2} &= \exp \left[ -\frac{1}{2} \frac{(\ln x_{1/2} - \mu_\mathrm{log})^2}{\sigma_\mathrm{log}^2} \right] \\
2 \ln 2 &= \frac{(\ln x_{1/2} - \mu_\mathrm{log})^2}{\sigma_\mathrm{log}^2} \\
\sqrt{2 \ln 2} \cdot \sigma_\mathrm{log} &= \pm (\ln x_{1/2} - \mu_\mathrm{log}) \\
x_1 &= \exp \left[ \mu_\mathrm{log} + \sqrt{2 \ln 2} \, \sigma_\mathrm{log} \right] \\
x_2 &= \exp \left[ \mu_\mathrm{log} - \sqrt{2 \ln 2} \, \sigma_\mathrm{log} \right] \; .
\end{split}
\end{equation}

With that, the FWHM tuning width is given by\footnote{This means that tuning width is transformed from logarithmic space to linear space in a different way than preferred numerosity. For an implementation, see here: \url{https://github.com/SkeideLab/EMPRISE/blob/akieslinger/data/code/harveymodel.py#L418} (function "sigma2fhwm").} 

\vspace{-0.5em}
\begin{equation} \label{eq:fwhm-qed}
\begin{split}
\omega &= x_1 - x_2 \\
\omega &= \exp \left[ \mu_\mathrm{log} + \sqrt{2 \ln 2} \, \sigma_\mathrm{log} \right] - \exp \left[ \mu_\mathrm{log} - \sqrt{2 \ln 2} \, \sigma_\mathrm{log} \right] \; .
\end{split}
\end{equation}

Note that $\mu$ can be inverted into $\mu_\mathrm{log}$ without knowing $\omega$

\begin{equation} \label{eq:mu-log-mu}
\mu = \exp \left[ \mu_\mathrm{log} \right] \quad \Leftrightarrow \quad \mu_\mathrm{log} = \ln \mu \; ,
\end{equation}

but $\omega$ cannot be inverted into $\sigma_\mathrm{log}$ without knowing $\mu$:

\vspace{-0.5em}
\begin{equation} \label{eq:sig-log-fwhm}
\begin{split}
\sigma_\mathrm{log} &= x(\mu, \omega), \quad \text{such that} \\
\omega &= \exp \left[ \ln \mu + \sqrt{2 \ln 2} \, x \right] - \exp \left[ \ln \mu - \sqrt{2 \ln 2} \, x \right] \; .
\end{split}
\end{equation}


\pagebreak
\setcounter{equation}{0}
\subsection{Equivalence of least squares and maximum likelihood} \label{sec:App-B}

Consider the multiple linear regression model with correlated observations\footnote{Note: For simplicity, we will proof the equivalence for the general case that the covariance of the errors is $\sigma^2 V$ (requiring weighted least squares, WLS) which includes the special case that $V = I_n$ (allowing for ordinary least squares, OLS).}:

\begin{equation} \label{eq:MLR}
y = X \beta + \varepsilon, \; \varepsilon \sim \mathcal{N}(0, \sigma^2 V) \; .
\end{equation}

After parameter estimated using weighted least squares

\begin{equation} \label{eq:MLR-WLS}
\hat{\beta} = (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \; ,
\end{equation}

the weighted residual sum of squares is:

\begin{equation} \label{eq:MLR-wRSS}
\mathrm{wRSS} = (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \; .
\end{equation}

For maximum likelihood estimation, equation \eqref{eq:MLR} entails the likelihood function

\vspace{-0.5em}
\begin{equation} \label{eq:MLR-LF}
\begin{split}
p(y|X,V,\beta,\sigma^2) &= \mathcal{N}(y; X\beta, \sigma^2 V) \\
&= \sqrt{\frac{1}{(2\pi)^n |\sigma^2 V|}} \cdot \exp\left[ -\frac{1}{2} (y - X\beta)^\mathrm{T} (\sigma^2 V)^{-1} (y - X\beta) \right]
\end{split}
\end{equation}

which leads to the following log-likelihood function:

\vspace{-0.5em}
\begin{equation} \label{eq:MLR-LLF}
\begin{split}
\mathrm{LL}(X,V,\beta,\sigma^2) &= \log p(y|X,V,\beta,\sigma^2) \\
&= - \frac{n}{2} \log(2\pi) - \frac{n}{2} \log (\sigma^2) - \frac{1}{2} \log|V| - \frac{1}{2 \sigma^2} (y - X\beta)^\mathrm{T} V^{-1} (y - X\beta) \; .
\end{split}
\end{equation}

Plugging the maximum likelihood estimates\footnote{For a derivation, see: \url{https://statproofbook.github.io/P/mlr-mle}.}

\vspace{-0.5em}
\begin{equation} \label{eq:MLR-MLE}
\begin{split}
\hat{\beta} &= (X^\mathrm{T} V^{-1} X)^{-1} X^\mathrm{T} V^{-1} y \\
\hat{\sigma}^2 &= \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})
\end{split}
\end{equation}

into the log-likelihood function, we obtain\footnote{For the derivation, see: \url{https://statproofbook.github.io/P/mlr-mll}.}:

\vspace{-0.5em}
\begin{equation} \label{eq:MLR-MLL}
\begin{split}
\mathrm{MLL}(X,V) &= \mathrm{LL}(X,V,\hat{\beta},\hat{\sigma}^2) \\
&= - \frac{n}{2} \log(2\pi) - \frac{n}{2} \log\left( \frac{1}{n} (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta}) \right) - \frac{1}{2} \log|V| - \frac{n \, (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})}{2 \, (y-X\hat{\beta})^\mathrm{T} V^{-1} (y-X\hat{\beta})} \\
&\overset{\eqref{eq:MLR-wRSS}}{=} - \frac{n}{2} \log(2\pi) - \frac{n}{2} \log\left( \mathrm{wRSS}/n \right) - \frac{1}{2} \log|V| - \frac{n \, \mathrm{wRSS}}{2 \, \mathrm{wRSS}} \\
&= - \frac{n}{2} \log(2\pi) - \frac{n}{2} \log\left( \mathrm{wRSS}/n \right) - \frac{n}{2} - \frac{1}{2} \log|V| \; .
\end{split}
\end{equation}

As one can see, $\mathrm{MLL}(X,V) \propto \log(\mathrm{wRSS})$. Thus, RSS-based estimation gives the same results as ML-based estimation.


\pagebreak
\setcounter{equation}{0}
\subsection{Relationship between maximum log-likelihood and R-squared} \label{sec:App-C}

The coefficient of determination, also referred to as "R-squared", is defined as

\begin{equation} \label{eq:R2}
R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}}
\end{equation}

where $\mathrm{RSS}$ and $\mathrm{TSS}$ are the residual and total sum of squares, respectively:

\vspace{-0.5em}
\begin{equation} \label{eq:RSS-TSS}
\begin{split}
\mathrm{RSS} &= \sum_{i=1}^n (y_i - \hat{y}_i)^2 \quad \text{where} \quad \hat{y} = X\hat{\beta} \quad \text{and} \quad \hat{\beta} = (X^\mathrm{T} X)^{-1} X^\mathrm{T} y \\
\mathrm{TSS} &= \sum_{i=1}^n (y_i - \bar{y})^2 \; .
\end{split}
\end{equation}

Now consider two multiple linear regression models, as given by equation (\ref{eq:y-X}), but characterized by different design matrices:

\vspace{-0.5em}
\begin{equation} \label{eq:m0-m1}
\begin{split}
m_0: \; X_0 &= 1_n \\
m_1: \; X_1 &= X
\end{split}
\end{equation}

First, we note that the total sum of squares is equal to the residual sum of squares for a model which only includes a constant regressor ($m_0$):

\begin{equation} \label{eq:TSS-m0}
\mathrm{TSS} = \mathrm{RSS}\left( X=1_n, \hat{\beta}=\bar{y} \right) \; .
\end{equation}

Second, we know that the maximum log-likelihood is a function of the residual sum of squares for a model with design matrix $X$ (cf. eq.~\ref{eq:MLL}):

\begin{equation} \label{eq:MLL-RSS}
\mathrm{MLL} = - \frac{n}{2} \log\left( \frac{\mathrm{RSS}}{n} \right) - \frac{n}{2} \log(2\pi) - \frac{n}{2} \; .
\end{equation}

Thus, we can calculate the maximum log-likelihood for the two models in (\ref{eq:m0-m1}) and subtract them from each other:

\vspace{-0.5em}
\begin{equation} \label{eq:dMLL}
\begin{split}
\Delta\mathrm{MLL} &= \mathrm{MLL}(m_1) - \mathrm{MLL}(m_0) \\
&= - \frac{n}{2} \log(\mathrm{RSS}) + \frac{n}{2} \log(\mathrm{TSS}) \; .
\end{split}
\end{equation}

Exponentiating both sides of the equation, taking both sides to the power of $-2/n$ and subtracting from 1, we obtain:

\vspace{-0.5em}
\begin{equation} \label{eq:MLL-R2}
\begin{split}
\exp[\Delta\mathrm{MLL}] &= \exp\left[ - \frac{n}{2} \log(\mathrm{RSS}) + \frac{n}{2} \log(\mathrm{TSS}) \right] \\
&= \left( \frac{\mathrm{RSS}}{\mathrm{TSS}} \right)^{-n/2} \\
\left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n} &= \left[ \left( \frac{\mathrm{RSS}}{\mathrm{TSS}} \right)^{-n/2} \right]^{-2/n} \\
&= \frac{\mathrm{RSS}}{\mathrm{TSS}} \\
1 - \left( \exp[\Delta\mathrm{MLL}] \right)^{-2/n} &= 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} = R^2 \; .
\end{split}
\end{equation}



%%% References %%%
\pagebreak
\setstretch{0.95}
\section{References}
\renewcommand{\section}[2]{}

\nocite{*}
\bibliographystyle{apacite}
\bibliography{References}


\end{document}